{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MonkeyMadness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start by enabling the GPU: (Deep learning requires many computations and the training time will be much faster if we do it on the GPU)\n",
    "1. Go to \"RunTime\" menu and select \n",
    "2. \"Change runtime type.\" A dialog box will appear where you can choose the runtime type and hardware accelerator.\n",
    "3. Select \"GPU\" as the hardware accelerator and click \"Save.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment and enable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Cizako/Classification-Game.git\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "%cd {HOME}/Classification-Game\n",
    "\n",
    "%pip install req_colab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "In this task, you will use supervised learning to classify images of monkeys. Supervised learning is one of the three main areas of machine learning.  \n",
    "\n",
    "Supervised learning is like teaching the computer to recognize monkeys by using flashcards.  \n",
    "\n",
    "1. **Labeled Data**: You show the computer images of monkeys and tell it which species each one is (e.g., \"This is a chimpanzee\").  \n",
    "2. **Training**: The computer studies these examples to learn patterns.  \n",
    "3. **Comparison**: It makes a prediction of the image and then compares it to the labeled data. It will be penalized based on how \"wrong\" it is.  \n",
    "4. **Prediction**: Once trained, it can look at a new image and guess the species of the monkey based on what it learned.  \n",
    "\n",
    "It’s called “supervised” because the model learns under guidance (the labeled data).  \n",
    "\n",
    "## There are some main ingredients:\n",
    "\n",
    "1. **Data**  \n",
    "    - The data has to be labeled, i.e., someone has to manually note down what monkey is present in the image.  \n",
    "\n",
    "2. **Model**  \n",
    "    - It is the model that makes predictions. It does so by looking at the input and making a guess about what monkey is present. The model consists of mathematical operations and weights (these are adjustable). In this task we will use so called Convolutional Neural Network which are very good at handling image data.\n",
    "\n",
    "3. **Loss Function**  \n",
    "    - Somehow we need to tell the computer how wrong the guess is.  \n",
    "    - The model will output probabilities for each monkey class. Let’s say it sees an image of an orangutan, then it could perhaps output the following:  \n",
    "\n",
    "        ```markdown\n",
    "        Chimpanzee: 0.70 (70%)  \n",
    "        Orangutan: 0.20 (20%)  \n",
    "        Other monkeys: 0.10 (10%)  \n",
    "        ```\n",
    "\n",
    "        The correct label is orangutan (100% probability), but the model guessed only 20% for this class.\n",
    "\n",
    "        Using a loss function like Cross-Entropy Loss, the score is calculated to show how wrong the prediction is\n",
    "        The closer the prediction is to 1 (100%), the smaller the loss. \n",
    "        The goal is to adjust the model to make higher confidence predictions for the correct class.\n",
    "4. **Optimizer**\n",
    "    - Somehow we need to adjust the model to perform better. This is done by calculating, based on the loss how the model should be adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a convolution? \n",
    "\n",
    "\n",
    "The convolution is like sliding a small \"window\" (called a kernel or filter) over an image to look for patterns.\n",
    "\n",
    "Here's how it works step-by-step:\n",
    "\n",
    "Kernel: Think of this as a small grid of numbers (e.g., 3x3).\n",
    "Slide and Multiply: Place the kernel on part of the image. Multiply the numbers in the kernel with the corresponding numbers in the image under it.\n",
    "Sum Up: Add the results of the multiplication together. This gives one number for that position.\n",
    "Move the Kernel: Slide the kernel to the next part of the image and repeat.\n",
    "The result is a new image (called a feature map) that highlights certain patterns like edges or textures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the image\n",
    "image_path = 'Monkey/training/training/n7/n7023.jpg'\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Transform the image to a tensor\n",
    "transform = transforms.ToTensor()\n",
    "image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Define a vertical line kernel\n",
    "# Define a vertical line kernel\n",
    "# Define vertical and horizontal line detection kernels\n",
    "vertical_line_kernel = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                     [-1.0, 0.0, 1.0],\n",
    "                                     [-1.0, 0.0, 1.0]], dtype=torch.float32)\n",
    "\n",
    "horizontal_line_kernel = torch.tensor([[-1.0, -1.0, -1.0],\n",
    "                                       [0.0,  0.0,  0.0],\n",
    "                                       [1.0,  1.0,  1.0]], dtype=torch.float32)\n",
    "\n",
    "# Expand kernels to match RGB channels\n",
    "vertical_line_kernel = vertical_line_kernel.expand(1, 3, 3, 3)\n",
    "horizontal_line_kernel = horizontal_line_kernel.expand(1, 3, 3, 3)\n",
    "\n",
    "# Apply the vertical and horizontal kernels to each channel\n",
    "vertical_convolved = F.conv2d(image_tensor, vertical_line_kernel, padding=1)\n",
    "horizontal_convolved = F.conv2d(image_tensor, horizontal_line_kernel, padding=1)\n",
    "\n",
    "\n",
    "# Convolve the image with the vertical line kernel\n",
    "\n",
    "# Remove the batch dimension and convert to a NumPy array for visualization\n",
    "vertical_convolved = vertical_convolved.squeeze().detach().numpy()\n",
    "horizontal_convolved = horizontal_convolved.squeeze().detach().numpy()\n",
    "# Plot the original and convolved images side by side\n",
    "# Plot the original image, vertical lines, and horizontal lines\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Original image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Vertical line convolution\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(vertical_convolved, cmap=\"gray\")\n",
    "plt.title(\"Vertical Line Convolution\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Horizontal line convolution\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(horizontal_convolved, cmap=\"gray\")\n",
    "plt.title(\"Horizontal Line Convolution\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset. \n",
    "### You will use a monkey dataset consisting of ca 1000 images of monkey. The goal is to be able to classify them.\n",
    "### There are ten monkey classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, utils\n",
    "from Dataset import CustomImageDataset, MonkeyImageDataset\n",
    "\n",
    "NUM_OF_CLASSES = 10\n",
    "IMAGE_SIZE = (64, 64)\n",
    "DATA_PERCENTAGE = 0.7\n",
    "transform = transforms.Compose([\n",
    "    #Randomly flip the images vertically\n",
    "    #transforms.RandomVerticalFlip(p=0.2),  # Randomly flip the image vertically with 20% probability\n",
    "    #transforms.RandomHorizontalFlip(p=0.2),  # Randomly flip the image horizontally with 20% probability\n",
    "    #transforms.RandomRotation(degrees=15),  # Rotate the image randomly within a 15-degree range\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly change brightness, contrast, etc.\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((IMAGE_SIZE[0], IMAGE_SIZE[1])), \n",
    "\n",
    "    #for imagenet\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "#dataset = CustomImageDataset('Monkey/training/training', transform=transform, amount_of_classes=NUM_OF_CLASSES, data_percentage=0.3)\n",
    "dataset = MonkeyImageDataset('Monkey/training/training', transform, NUM_OF_CLASSES, data_percentage = DATA_PERCENTAGE )\n",
    "dataset.visualize(5)\n",
    "\n",
    "\n",
    "dataset.visualize_all_classes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's define our model! \n",
    "### Lets build a Convolutional Neural Network (CNN). It uses convolutions (one can think of it as filters) to learn the different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class MonkeyNET(nn.Module):\n",
    "    def __init__(self, num_classes=10, input_size=(500, 500), dropout_rate=0.5):\n",
    "        super(MonkeyNET, self).__init__()\n",
    "        \n",
    "        # First convolutional layer: 3 input channels (RGB), 32 output channels, kernel size 5, padding 2 to preserve size\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2)\n",
    "        \n",
    "        # Second convolutional layer: outputs a 32-channel feature map\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        \n",
    "        ## Third convolutional layer: further reduces spatial dimensions\n",
    "        #self.conv3 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout2d(p=dropout_rate)  # Spatial dropout for convolutional layers\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.dropout3 = nn.Dropout(p=dropout_rate)    # Regular dropout for fully connected layers\n",
    "\n",
    "\n",
    "        # Calculate the size of the fully connected layer dynamically\n",
    "        self.fc_input_size = self._get_fc_input_size(input_size)\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 64)  # Adjusted for the final size after pooling\n",
    "        \n",
    "        # Prediction layer\n",
    "        self.prediction = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def _get_fc_input_size(self, input_size):\n",
    "        x = torch.zeros(1, 3, *input_size)  # Create a dummy input tensor\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        #x = F.relu(self.conv3(x))\n",
    "        #x = F.max_pool2d(x, kernel_size=2)\n",
    "        return x.numel()  # Total number of elements after conv layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First conv -> ReLU -> Max Pooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout1(x)  # Apply spatial dropout\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        #Second conv -> ReLU -> Max Pooling\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # Third conv -> ReLU -> Max Pooling\n",
    "        #x = F.relu(self.conv3(x))\n",
    "        #x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "\n",
    "        # Flatten the tensor for fully connected layer\n",
    "        x = x.view(x.size(0), -1)  # Output: (batch_size, 128 * 16 * 16) for 500x500 input\n",
    "\n",
    "        # Fully connected layer -> ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)  # Apply regular dropout before final layer\n",
    "\n",
    "\n",
    "        # Output layer (no activation, to be combined with a loss function later)\n",
    "        x = self.prediction(x)\n",
    "        # Optionally remove Softmax from here\n",
    "        return x\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See the summary of the model in a compact way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import torchlens as tl\n",
    "import graphviz\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "\n",
    "model = MonkeyNET(num_classes=NUM_OF_CLASSES, input_size=IMAGE_SIZE)\n",
    "\n",
    "summary(model, (3, IMAGE_SIZE[0], IMAGE_SIZE[1]))\n",
    "\n",
    "# ---- Uncomment the lines below to get a visual of the model ------\n",
    "# graphviz.set_jupyter_format('png')\n",
    "# x = torch.rand(1, 3, IMAGE_SIZE[0], IMAGE_SIZE[1])\n",
    "# model_hist = tl.log_forward_pass(model,x, vis_opt='unrolled')\n",
    "# model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train, training_info\n",
    "from test_model import test\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA (GPU)\")\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders for macOS) is available (for Apple Silicon Macs)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple GPU)\")\n",
    "\n",
    "# Fallback to CPU if neither CUDA nor MPS is available\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "START_EPOCH = 0\n",
    "EPOCHS = 0 # how long should the model train\n",
    "MODEL_NAME = \"drop_0.5 two layers\"\n",
    "ID = 2213\n",
    "LR = 0.01 # how \"much\" should the model learn\n",
    "BATCH_SIZE = 8# how many images should the model see before updating\n",
    "\n",
    "model_info = {\n",
    "            'epochs' : EPOCHS,\n",
    "            'batch_size' : BATCH_SIZE,\n",
    "            'lr' : LR,\n",
    "            'ID' : ID,\n",
    "            'model_name' : MODEL_NAME\n",
    "\n",
    "}\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)  # 80% for training\n",
    "val_size = dataset_size - train_size   # 20% for validation\n",
    "if NUM_OF_CLASSES > 2:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "print(train_size, val_size)\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "model1 = MonkeyNET(num_classes=NUM_OF_CLASSES, input_size=IMAGE_SIZE, dropout_rate=0.1)\n",
    "\n",
    "\n",
    "# Create optimizers for the model. This will try to find the optimal parameters in the model i.e this adjusts the model to improve the loss\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=LR)\n",
    "\n",
    "# Move models to gpu and train \n",
    "model1.to(device)\n",
    "model1, t_loss, t_acc, v_loss, v_acc = train(model1, train_loader, val_loader, optimizer1, criterion, device, start_epoch=START_EPOCH, num_epochs=EPOCHS, model_name=MODEL_NAME, unique_id=ID)\n",
    "\n",
    "test_dataset = MonkeyImageDataset('Monkey/validation/validation', transform, NUM_OF_CLASSES, 1)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "model_info['t_loss'] = t_loss\n",
    "model_info['t_acc'] = t_acc\n",
    "\n",
    "model_info['v_loss'] = v_loss\n",
    "model_info['v_acc'] = v_acc\n",
    "\n",
    "training_info(model_info=model_info)\n",
    "\n",
    "# see metrics on the validation set\n",
    "\n",
    "acc = test(model=model1, testloader=val_loader, device=device, model_name=MODEL_NAME, unique_id=ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you are happy with the results you can save the model. Give it a good name like your group name and a brief description e.g 'GroupBananasBest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Assuming model1 is defined and trained\n",
    "save_model_name = \"second\"\n",
    "\n",
    "# Define the directory to save the model\n",
    "model_dir = 'saved_models'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model1.eval()\n",
    "\n",
    "# Define a dummy input tensor matching the input shape expected by the model\n",
    "# Here we assume the model expects a 1x3x224x224 image (batch size 1, 3 channels, 224x224 pixels)\n",
    "# You should adjust the shape based on your actual model's input\n",
    "dummy_input = torch.randn(1, 3, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "onnx_path = f\"{model_dir}/{save_model_name}.onnx\"\n",
    "torch.onnx.export(model1, dummy_input, onnx_path, verbose=True)\n",
    "\n",
    "print(f\"Model saved to {onnx_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See all performed experiments to compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from train import plot_experiments\n",
    "\n",
    "plot_experiments('training_metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send your best model to us for us to run it on a test set!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
